{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "from ebooklib import epub\n",
    "import random, pickle, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/portuguese_sentences.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carolina = load_dataset('carolina-c4ai/corpus-carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carolina_text = carolina['corpus']['text']\n",
    "print(len(carolina_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\.|\\?|!|;|\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_books = ['a_guerra_dos_tronos','linha_d_agua','o_alienista', 'ensaio_sobre_a_cegueira', 'sapiens', 'o_guarani', 'colecao_especial_jane_austen', 'o_livro_das_princesas','a_falencia', 'sob_a_redoma', 'os_cem_melhores_contos_brasileiros_do_seculo']\n",
    "list_books = ['os_tres_mosqueteiros', 'harry_potter_e_a_ordem_da_fenix', 'grande_sertao_veredas', 'a_redoma_de_vidro', 'aristoteles_e_dante_descobrem_os_segredos_do_universo', 'como_evitar_preocupacoes_e_comecar_a_viver']\n",
    "list_books = [book+'.epub' for book in list_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book(book_name):\n",
    "    book = epub.read_epub(f'data/epubs/{book_name}')\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    def chapter_to_str(chapter):\n",
    "        soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
    "        text = [para.get_text() for para in soup.find_all('p')]\n",
    "        return ''.join(text)\n",
    "    texts = \"\"\n",
    "    for c in items:\n",
    "        chapter = chapter_to_str(c)\n",
    "        texts += chapter\n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ' '.join([process_book(book) for book in list_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(regex, raw_text)\n",
    "sentences += carolina_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence.replace('\\n', ' ')\n",
    "\n",
    "    while sentence and sentence[0] in ['.', ',', ':', '!', '?', ';']:\n",
    "        sentence = sentence[1:]\n",
    "    # fix whitespaces\n",
    "    while '  ' in sentence:\n",
    "        sentence = sentence.replace('  ', ' ')\n",
    "    if sentence and sentence[0] == ' ':\n",
    "        sentence = sentence[1:]\n",
    "    if sentence and sentence[-1] == ' ':\n",
    "        sentence = sentence[:-1]\n",
    "    return sentence\n",
    "    \n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [s for s in sentences if len(s) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=set()\n",
    "print(f'Size with duplicates: {len(sentences)}')\n",
    "for s in sentences:\n",
    "    duplicates.add(s) \n",
    "sentences = list(duplicates)\n",
    "print(f'Size without duplicates: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'w') as file:\n",
    "    file.write('\\n'.join(sentences))\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate annotated data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for generating similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_adjacent_letters_pt = {\n",
    "    'a': ['s', 'z', 'q', 'w', 'á', 'à', 'â', 'ã'],\n",
    "    'b': ['v', 'g', 'n', 'h'],\n",
    "    'c': ['x', 'd', 'v', 'f', 'ç'],\n",
    "    'd': ['s', 'e', 'c', 'x', 'f', 'r'],\n",
    "    'e': ['w', 'r', 'd', 's', 'é', 'ê'],\n",
    "    'f': ['d', 'r', 'g', 'v', 'c', 't'],\n",
    "    'g': ['f', 't', 'h', 'b', 'v', 'r'],\n",
    "    'h': ['g', 't', 'j', 'n', 'b', 'y'],\n",
    "    'i': ['u', 'o', 'k', 'j', 'í'],\n",
    "    'j': ['h', 'y', 'k', 'n', 'm', 'u', 'i'],\n",
    "    'k': ['j', 'i', 'l', 'm', 'o', 'n'],\n",
    "    'l': ['k', 'o', 'p', 'm'],\n",
    "    'm': ['n', 'j', 'k', 'l'],\n",
    "    'n': ['b', 'h', 'j', 'm'],\n",
    "    'o': ['i', 'p', 'l', 'k', 'ó', 'ô', 'õ'],\n",
    "    'p': ['o', 'l', 'ç'],\n",
    "    'q': ['a', 'z', 'u'],\n",
    "    'r': ['e', 't', 'f', 'd', 'r'],\n",
    "    's': ['a', 'w', 'e', 'd', 'x', 'z'],\n",
    "    't': ['r', 'y', 'g', 'f'],\n",
    "    'u': ['y', 'j', 'i', 'h', 'ú'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'a', 's', 'e'],\n",
    "    'x': ['z', 's', 'd', 'c'],\n",
    "    'y': ['t', 'u', 'h', 'g'],\n",
    "    'z': ['x', 's', 'a', 'ç'],\n",
    "    'ç': ['c'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_strings(str, x = None, adjacent_letters = True):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of similar strings,\n",
    "    all in lowercase, according to the following rules:\n",
    "\n",
    "    * if 'x' is None, it will be:\n",
    "      -> 1, if len(str) <= 6\n",
    "      -> 2, if len(str) <= 12\n",
    "      -> 3, if len(str) > 12\n",
    "    * all strings will be common Portuguese cognitive erros or\n",
    "      strings 'x' edits away from str, where an edit is:\n",
    "      -> insert a letter\n",
    "      -> delete a letter\n",
    "      -> replace one letter, and the letter will be any letter in the Portuguese alphabet or \n",
    "         just the adjacent letters in the keyboard if the flag\n",
    "         'adjacent_letters' is set to true.\n",
    "    \"\"\"\n",
    "    str = str.lower()\n",
    "\n",
    "    if x is None:\n",
    "        for edits, size in [(1, 1000)]:\n",
    "            if len(str) <= size:\n",
    "                x = edits \n",
    "                break\n",
    "\n",
    "    ALPHABET_UPPER = 'ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÃÇÉÊÍÓÔÕÚàáâãçéêíóôõú'\n",
    "    ALPHABET_LOWER = ALPHABET_UPPER.lower()\n",
    "\n",
    "    def concatenate_function(func, n):\n",
    "        if n == 1:\n",
    "            return func\n",
    "        return lambda x: func(concatenate_function(func, n-1)(x))\n",
    "    \n",
    "    def insert(words):\n",
    "        \"\"\"\n",
    "        Receives an iterable of words and returns\n",
    "        a set with all the possible insertions of each word.\n",
    "        \"\"\"\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for pos in range(len(str)+1):\n",
    "                left = str[:pos]\n",
    "                right = str[pos:]\n",
    "                \n",
    "                for char in ALPHABET_LOWER:\n",
    "                    return_words.add(left+char+right)\\\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def delete(words):\n",
    "        return_words = set()\n",
    "        \n",
    "        for str in words:\n",
    "            if len(str) <= 1:\n",
    "                continue \n",
    "            for i in range(len(str)):\n",
    "                left = str[:i]\n",
    "                right = str[i+1:]\n",
    "                return_words.add(left+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def replace(words):\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for ix, char in enumerate(str):\n",
    "                left = str[:ix]\n",
    "                right = str[ix+1:]\n",
    "                for c in ALPHABET_LOWER:\n",
    "                    return_words.add(left+c+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    all_edits = set()\n",
    "\n",
    "    for func in [insert, delete, replace]:\n",
    "        conc_func = concatenate_function(func, x)\n",
    "        all_edits = all_edits | conc_func({str})\n",
    "\n",
    "    for ix,c in enumerate(str):\n",
    "        all_edits = all_edits | {str[:ix]+c.swapcase()+str[ix+1:]}\n",
    "\n",
    "    # common Portuguese errors\n",
    "    # ss and ç\n",
    "    all_edits.add(str.replace('ss', 'ç'))\n",
    "    all_edits.add(str.replace('ç', 'ss'))\n",
    "\n",
    "    # ão and am\n",
    "    all_edits.add(str.replace('ão', 'am'))\n",
    "    all_edits.add(str.replace('am', 'ão'))\n",
    "    \n",
    "    all_edits.discard(str)\n",
    "    \n",
    "    return all_edits\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    for s in file:\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dicionario.pickle', 'rb') as file:\n",
    "    loaded_df = pickle.load(file)\n",
    "\n",
    "words = []\n",
    "freq_dic = {}\n",
    "\n",
    "for word, freq in zip(loaded_df['word'], loaded_df['frequency']):\n",
    "    words.append((-freq, word))\n",
    "    freq_dic[word] = freq\n",
    "\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_mistakes(word, just_similar = False):\n",
    "    if word not in freq_dic:\n",
    "        return []\n",
    "\n",
    "    similar_words = get_similar_strings(word)\n",
    "\n",
    "    if just_similar:\n",
    "        return similar_words\n",
    "\n",
    "    mistakes = []\n",
    "\n",
    "    for similar in similar_words:\n",
    "        if freq_dic.get(similar, 10_000_000) <= (freq_dic[word]/5):\n",
    "            mistakes.append(similar)\n",
    "    mistakes.sort(key=lambda x: freq_dic[x], reverse=True)\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_possible_mistakes('você', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_number(left, right):\n",
    "    \"\"\"\n",
    "    Requires left <= right\n",
    "    \"\"\"\n",
    "    num = random.random() * (right-left)\n",
    "    num = round(num)\n",
    "    return num+left \n",
    "\n",
    "def draw_random_quantity():\n",
    "    qtd_array = [0,0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,4]\n",
    "    random_index = draw_random_number(0,len(qtd_array)-1)\n",
    "    return qtd_array[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_number(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'wrong_text': [], 'correct_text': []}\n",
    "\n",
    "wrong_text = df['wrong_text']\n",
    "correct_text = df['correct_text']\n",
    "duplicates = set()\n",
    "\n",
    "for s in tqdm(sentences):\n",
    "    s = s.replace('\\n', '')\n",
    "    \n",
    "    for i in range(1):\n",
    "        #to_mess_up = draw_random_quantity()\n",
    "        to_mess_up = 1\n",
    "\n",
    "        curr_s = s\n",
    "\n",
    "        times_run = 10\n",
    "\n",
    "        while to_mess_up and times_run:\n",
    "            times_run -= 1\n",
    "            all_matches = list(re.finditer('\\w+', curr_s))\n",
    "\n",
    "            if len(all_matches) == 0:\n",
    "                break\n",
    "\n",
    "            random_ix = draw_random_number(0,len(all_matches)-1)\n",
    "            match=all_matches[random_ix]\n",
    "            \n",
    "            mistakes = get_possible_mistakes(match.group())\n",
    "            mistakes = mistakes[:5]\n",
    "            random.shuffle(mistakes)\n",
    "            \n",
    "            if not mistakes:\n",
    "                continue \n",
    "\n",
    "            beg = match.start()\n",
    "            en = match.end()\n",
    "\n",
    "            random.shuffle(mistakes)\n",
    "\n",
    "            for mistake in mistakes: \n",
    "                aux = curr_s[:beg]+mistake+curr_s[en:]\n",
    "                if aux in duplicates:\n",
    "                    continue \n",
    "                duplicates.add(aux)\n",
    "                wrong_text.append(aux)\n",
    "                correct_text.append(s)\n",
    "\n",
    "            #curr_s = curr_s[:beg]+mistakes[0]+curr_s[en:]\n",
    "\n",
    "            to_mess_up -= 1\n",
    "        \n",
    "        if curr_s in duplicates:\n",
    "            continue \n",
    "        \n",
    "        wrong_text.append(curr_s)\n",
    "        correct_text.append(s)\n",
    "        duplicates.add(curr_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['wrong_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated_data.pickle', 'wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
