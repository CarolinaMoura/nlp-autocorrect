{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolmou/linguas-indigenas/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "from ebooklib import epub\n",
    "import random, pickle, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/portuguese_sentences.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset corpus-carolina (/home/carolmou/.cache/huggingface/datasets/carolina-c4ai___corpus-carolina/carolina/1.2.0/60fe73ac1719891e34135322031692bf177e9323e830d620cf3304f535ee2693)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "carolina = load_dataset('carolina-c4ai/corpus-carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2107045\n"
     ]
    }
   ],
   "source": [
    "carolina_text = carolina['corpus']['text']\n",
    "print(len(carolina_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\.|\\?|!|;|\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_books = ['a_guerra_dos_tronos','linha_d_agua','o_alienista', 'ensaio_sobre_a_cegueira', 'sapiens', 'o_guarani', 'colecao_especial_jane_austen', 'o_livro_das_princesas','a_falencia', 'sob_a_redoma', 'os_cem_melhores_contos_brasileiros_do_seculo']\n",
    "list_books = ['os_tres_mosqueteiros', 'harry_potter_e_a_ordem_da_fenix', 'grande_sertao_veredas', 'a_redoma_de_vidro', 'aristoteles_e_dante_descobrem_os_segredos_do_universo', 'como_evitar_preocupacoes_e_comecar_a_viver']\n",
    "list_books = [book+'.epub' for book in list_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book(book_name):\n",
    "    book = epub.read_epub(f'data/epubs/{book_name}')\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    def chapter_to_str(chapter):\n",
    "        soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
    "        text = [para.get_text() for para in soup.find_all('p')]\n",
    "        return ''.join(text)\n",
    "    texts = \"\"\n",
    "    for c in items:\n",
    "        chapter = chapter_to_str(c)\n",
    "        texts += chapter\n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolmou/linguas-indigenas/.venv/lib/python3.10/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "raw_text = ' '.join([process_book(book) for book in list_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(regex, raw_text)\n",
    "sentences += carolina_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence.replace('\\n', ' ')\n",
    "\n",
    "    while sentence and sentence[0] in ['.', ',', ':', '!', '?', ';']:\n",
    "        sentence = sentence[1:]\n",
    "    # fix whitespaces\n",
    "    while '  ' in sentence:\n",
    "        sentence = sentence.replace('  ', ' ')\n",
    "    if sentence and sentence[0] == ' ':\n",
    "        sentence = sentence[1:]\n",
    "    if sentence and sentence[-1] == ' ':\n",
    "        sentence = sentence[:-1]\n",
    "    return sentence\n",
    "    \n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [s for s in sentences if len(s) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size with duplicates: 2184693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size without duplicates: 2149297\n"
     ]
    }
   ],
   "source": [
    "duplicates=set()\n",
    "print(f'Size with duplicates: {len(sentences)}')\n",
    "for s in sentences:\n",
    "    duplicates.add(s) \n",
    "sentences = list(duplicates)\n",
    "print(f'Size without duplicates: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'w') as file:\n",
    "    file.write('\\n'.join(sentences))\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate annotated data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for generating similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_adjacent_letters_pt = {\n",
    "    'a': ['s', 'z', 'q', 'w', 'á', 'à', 'â', 'ã'],\n",
    "    'b': ['v', 'g', 'n', 'h'],\n",
    "    'c': ['x', 'd', 'v', 'f', 'ç'],\n",
    "    'd': ['s', 'e', 'c', 'x', 'f', 'r'],\n",
    "    'e': ['w', 'r', 'd', 's', 'é', 'ê'],\n",
    "    'f': ['d', 'r', 'g', 'v', 'c', 't'],\n",
    "    'g': ['f', 't', 'h', 'b', 'v', 'r'],\n",
    "    'h': ['g', 't', 'j', 'n', 'b', 'y'],\n",
    "    'i': ['u', 'o', 'k', 'j', 'í'],\n",
    "    'j': ['h', 'y', 'k', 'n', 'm', 'u', 'i'],\n",
    "    'k': ['j', 'i', 'l', 'm', 'o', 'n'],\n",
    "    'l': ['k', 'o', 'p', 'm'],\n",
    "    'm': ['n', 'j', 'k', 'l'],\n",
    "    'n': ['b', 'h', 'j', 'm'],\n",
    "    'o': ['i', 'p', 'l', 'k', 'ó', 'ô', 'õ'],\n",
    "    'p': ['o', 'l', 'ç'],\n",
    "    'q': ['a', 'z', 'u'],\n",
    "    'r': ['e', 't', 'f', 'd', 'r'],\n",
    "    's': ['a', 'w', 'e', 'd', 'x', 'z'],\n",
    "    't': ['r', 'y', 'g', 'f'],\n",
    "    'u': ['y', 'j', 'i', 'h', 'ú'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'a', 's', 'e'],\n",
    "    'x': ['z', 's', 'd', 'c'],\n",
    "    'y': ['t', 'u', 'h', 'g'],\n",
    "    'z': ['x', 's', 'a', 'ç'],\n",
    "    'ç': ['c'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_strings(str, x = None, adjacent_letters = True):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of similar strings,\n",
    "    all in lowercase, according to the following rules:\n",
    "\n",
    "    * if 'x' is None, it will be:\n",
    "      -> 1, if len(str) <= 6\n",
    "      -> 2, if len(str) <= 12\n",
    "      -> 3, if len(str) > 12\n",
    "    * all strings will be common Portuguese cognitive erros or\n",
    "      strings 'x' edits away from str, where an edit is:\n",
    "      -> insert a letter\n",
    "      -> delete a letter\n",
    "      -> replace one letter, and the letter will be any letter in the Portuguese alphabet or \n",
    "         just the adjacent letters in the keyboard if the flag\n",
    "         'adjacent_letters' is set to true.\n",
    "    \"\"\"\n",
    "    str = str.lower()\n",
    "\n",
    "    if x is None:\n",
    "        for edits, size in [(1, 1000)]:\n",
    "            if len(str) <= size:\n",
    "                x = edits \n",
    "                break\n",
    "\n",
    "    ALPHABET_UPPER = 'ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÃÇÉÊÍÓÔÕÚàáâãçéêíóôõú'\n",
    "    ALPHABET_LOWER = ALPHABET_UPPER.lower()\n",
    "\n",
    "    def concatenate_function(func, n):\n",
    "        if n == 1:\n",
    "            return func\n",
    "        return lambda x: func(concatenate_function(func, n-1)(x))\n",
    "    \n",
    "    def insert(words):\n",
    "        \"\"\"\n",
    "        Receives an iterable of words and returns\n",
    "        a set with all the possible insertions of each word.\n",
    "        \"\"\"\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for pos in range(len(str)+1):\n",
    "                left = str[:pos]\n",
    "                right = str[pos:]\n",
    "                \n",
    "                for char in ALPHABET_LOWER:\n",
    "                    return_words.add(left+char+right)\\\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def delete(words):\n",
    "        return_words = set()\n",
    "        \n",
    "        for str in words:\n",
    "            if len(str) <= 1:\n",
    "                continue \n",
    "            for i in range(len(str)):\n",
    "                left = str[:i]\n",
    "                right = str[i+1:]\n",
    "                return_words.add(left+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def replace(words):\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for ix, char in enumerate(str):\n",
    "                left = str[:ix]\n",
    "                right = str[ix+1:]\n",
    "                for c in ALPHABET_LOWER:\n",
    "                    return_words.add(left+c+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    all_edits = set()\n",
    "\n",
    "    for func in [insert, delete, replace]:\n",
    "        conc_func = concatenate_function(func, x)\n",
    "        all_edits = all_edits | conc_func({str})\n",
    "\n",
    "    for ix,c in enumerate(str):\n",
    "        all_edits = all_edits | {str[:ix]+c.swapcase()+str[ix+1:]}\n",
    "\n",
    "    # common Portuguese errors\n",
    "    # ss and ç\n",
    "    all_edits.add(str.replace('ss', 'ç'))\n",
    "    all_edits.add(str.replace('ç', 'ss'))\n",
    "\n",
    "    # ão and am\n",
    "    all_edits.add(str.replace('ão', 'am'))\n",
    "    all_edits.add(str.replace('am', 'ão'))\n",
    "    \n",
    "    all_edits.discard(str)\n",
    "    \n",
    "    return all_edits\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    for s in file:\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dicionario.pickle', 'rb') as file:\n",
    "    loaded_df = pickle.load(file)\n",
    "\n",
    "words = []\n",
    "freq_dic = {}\n",
    "\n",
    "for word, freq in zip(loaded_df['word'], loaded_df['frequency']):\n",
    "    words.append((-freq, word))\n",
    "    freq_dic[word] = freq\n",
    "\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_mistakes(word, just_similar = False):\n",
    "    if word not in freq_dic:\n",
    "        return []\n",
    "\n",
    "    similar_words = get_similar_strings(word)\n",
    "\n",
    "    if just_similar:\n",
    "        return similar_words\n",
    "\n",
    "    mistakes = []\n",
    "\n",
    "    for similar in similar_words:\n",
    "        if freq_dic.get(similar, 10_000_000) <= (freq_dic[word]/5):\n",
    "            mistakes.append(similar)\n",
    "    mistakes.sort(key=lambda x: freq_dic[x], reverse=True)\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vodcê', 'vçcê', 'voãcê', 'mvocê', 'focê', 'vwocê', 'vicê', 'vocêw', 'vnocê', 'tocê', 'vbcê', 'voõcê', 'vofcê', 'tvocê', 'vocéê', 'voôê', 'vocm', 'vocêà', 'vocêõ', 'vocçê', 'voacê', 'véocê', 'vocÊ', 'vock', 'ávocê', 'vobcê', 'xvocê', 'voê', 'ivocê', 'vociê', 'voêê', 'vúcê', 'vfocê', 'vocêã', 'vêcê', 'wvocê', 'àvocê', 'vocêt', 'oocê', 'çocê', 'vodê', 'cvocê', 'vocàê', 'svocê', 'vocêá', 'vfcê', 'voeê', 'võcê', 'àocê', 'vocêd', 'vocêh', 'vkocê', 'volcê', 'vocbê', 'vocêp', 'voch', 'vocdê', 'vocêm', 'rocê', 'vocgê', 'vâcê', 'bvocê', 'voácê', 'ãvocê', 'vtocê', 'vocêí', 'vorcê', 'voclê', 'ôocê', 'vocêê', 'voscê', 'váocê', 'voàê', 'evocê', 'íocê', 'vocêb', 'vàcê', 'vonê', 'vogê', 'vaocê', 'gvocê', 'vxocê', 'vocs', 'nocê', 'vocêg', 'évocê', 'vocà', 'vgocê', 'vocêj', 'jvocê', 'voczê', 'vôocê', 'vocrê', 'zocê', 'vocên', 'xocê', 'vocêú', 'vcocê', 'vocoê', 'yvocê', 'vâocê', 'voicê', 'vocêk', 'vocú', 'vvcê', 'vocêe', 'voct', 'vôcê', 'vocâ', 'avocê', 'vocõê', 'voaê', 'vocíê', 'vOcê', 'nvocê', 'vopê', 'docê', 'vopcê', 'vocêç', 'vsocê', 'vocêo', 'õocê', 'vocb', 'fvocê', 'voiê', 'voxê', 'veocê', 'vocês', 'vozê', 'vcê', 'hocê', 'vvocê', 'voçê', 'vocô', 'vowcê', 'vocl', 'voáê', 'voccê', 'voâcê', 'zvocê', 'vocf', 'vzocê', 'yocê', 'vosê', 'vogcê', 'uocê', 'voco', 'hvocê', 'voucê', 'vpcê', 'viocê', 'vlocê', 'eocê', 'gocê', 'vokcê', 'úocê', 'vocpê', 'aocê', 'vocp', 'voócê', 'vocêr', 'vocêu', 'voúê', 'voóê', 'vecê', 'Você', 'lvocê', 'bocê', 'vocg', 'voca', 'qocê', 'voyê', 'cocê', 'éocê', 'vocôê', 'vocqê', 'vojê', 'pocê', 'volê', 'vocr', 'vycê', 'voõê', 'vocúê', 'úvocê', 'voxcê', 'óvocê', 'vohê', 'vçocê', 'vocç', 'voàcê', 'vocx', 'voícê', 'dvocê', 'vocêa', 'çvocê', 'voçcê', 'vlcê', 'voecê', 'vockê', 'vocêy', 'vocóê', 'vobê', 'vóocê', 'voãê', 'uvocê', 'vocêô', 'vocd', 'vqcê', 'vtcê', 'voúcê', 'vooê', 'voécê', 'vocxê', 'vocêq', 'vgcê', 'vowê', 'vocu', 'vorê', 'ocê', 'vocáê', 'vocq', 'êvocê', 'wocê', 'vouê', 'vovê', 'mocê', 'voctê', 'kvocê', 'vocvê', 'vdocê', 'kocê', 'vícê', 'pvocê', 'óocê', 'vocêc', 'vocêv', 'êocê', 'vúocê', 'voCê', 'vrcê', 'õvocê', 'vkcê', 'vbocê', 'vqocê', 'vofê', 'vocõ', 'vccê', 'voâê', 'vocã', 'vêocê', 'vrocê', 'vocêâ', 'locê', 'ôvocê', 'voci', 'voôcê', 'voceê', 'vocâê', 'vdcê', 'vscê', 'vohcê', 'vocuê', 'voncê', 'votcê', 'vocc', 'vwcê', 'vocv', 'vocêl', 'vmocê', 'vyocê', 'âocê', 'jocê', 'votê', 'ovocê', 'võocê', 'voqê', 'víocê', 'vojcê', 'vxcê', 'vjocê', 'vucê', 'vokê', 'vocé', 'vocjê', 'voíê', 'voqcê', 'vocfê', 'vocj', 'vmcê', 'vocn', 'ívocê', 'vochê', 'vocêi', 'vomcê', 'vocmê', 'vocêé', 'voéê', 'vocw', 'vocz', 'vàocê', 'voêcê', 'âvocê', 'ãocê', 'vócê', 'vocêx', 'voocê', 'voycê', 'qvocê', 'vhocê', 'vocnê', 'voc', 'vocá', 'vocó', 'vocy', 'vocêz', 'vocwê', 'vocaê', 'vozcê', 'vomê', 'áocê', 'vácê', 'vacê', 'voce', 'vzcê', 'vpocê', 'vãcê', 'vovcê', 'vocyê', 'rvocê', 'vocsê', 'vhcê', 'vãocê', 'iocê', 'socê', 'vjcê', 'vécê', 'vocãê', 'vuocê', 'vocí', 'vncê', 'vocêó', 'vocêf'}\n"
     ]
    }
   ],
   "source": [
    "print(get_possible_mistakes('você', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_number(left, right):\n",
    "    \"\"\"\n",
    "    Requires left <= right\n",
    "    \"\"\"\n",
    "    num = random.random() * (right-left)\n",
    "    num = round(num)\n",
    "    return num+left \n",
    "\n",
    "def draw_random_quantity():\n",
    "    qtd_array = [0,0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,4]\n",
    "    random_index = draw_random_number(0,len(qtd_array)-1)\n",
    "    return qtd_array[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_random_number(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73488/73488 [00:17<00:00, 4256.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df = {'wrong_text': [], 'correct_text': []}\n",
    "\n",
    "wrong_text = df['wrong_text']\n",
    "correct_text = df['correct_text']\n",
    "duplicates = set()\n",
    "\n",
    "for s in tqdm(sentences):\n",
    "    s = s.replace('\\n', '')\n",
    "    \n",
    "    for i in range(1):\n",
    "        #to_mess_up = draw_random_quantity()\n",
    "        to_mess_up = 1\n",
    "\n",
    "        curr_s = s\n",
    "\n",
    "        times_run = 10\n",
    "\n",
    "        while to_mess_up and times_run:\n",
    "            times_run -= 1\n",
    "            all_matches = list(re.finditer('\\w+', curr_s))\n",
    "\n",
    "            if len(all_matches) == 0:\n",
    "                break\n",
    "\n",
    "            random_ix = draw_random_number(0,len(all_matches)-1)\n",
    "            match=all_matches[random_ix]\n",
    "            \n",
    "            mistakes = get_possible_mistakes(match.group())\n",
    "            mistakes = mistakes[:5]\n",
    "            random.shuffle(mistakes)\n",
    "            \n",
    "            if not mistakes:\n",
    "                continue \n",
    "\n",
    "            beg = match.start()\n",
    "            en = match.end()\n",
    "\n",
    "            random.shuffle(mistakes)\n",
    "\n",
    "            for mistake in mistakes: \n",
    "                aux = curr_s[:beg]+mistake+curr_s[en:]\n",
    "                if aux in duplicates:\n",
    "                    continue \n",
    "                duplicates.add(aux)\n",
    "                wrong_text.append(aux)\n",
    "                correct_text.append(s)\n",
    "\n",
    "            #curr_s = curr_s[:beg]+mistakes[0]+curr_s[en:]\n",
    "\n",
    "            to_mess_up -= 1\n",
    "        \n",
    "        if curr_s in duplicates:\n",
    "            continue \n",
    "        \n",
    "        wrong_text.append(curr_s)\n",
    "        correct_text.append(s)\n",
    "        duplicates.add(curr_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413976"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['wrong_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wrong_text</th>\n",
       "      <th>correct_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu sorria bastante, mas nunca sabia aí certo o...</td>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ao certo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ano certo ...</td>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ao certo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eu sorria bastante, mas nunca sabia co certo o...</td>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ao certo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eu sorria bastante, mas nunca sabia so certo o...</td>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ao certo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eu sorria bastante, mas nunca sabia Ao certo o...</td>\n",
       "      <td>Eu sorria bastante, mas nunca sabia ao certo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413971</th>\n",
       "      <td>Bem, só Deus criara as luzes verdes, por que n...</td>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413972</th>\n",
       "      <td>Bem, Se Deus criara as luzes verdes, por que n...</td>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413973</th>\n",
       "      <td>Bem, s Deus criara as luzes verdes, por que nã...</td>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413974</th>\n",
       "      <td>Bem, sei Deus criara as luzes verdes, por que ...</td>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413975</th>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "      <td>Bem, se Deus criara as luzes verdes, por que n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413976 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               wrong_text  \\\n",
       "0       Eu sorria bastante, mas nunca sabia aí certo o...   \n",
       "1       Eu sorria bastante, mas nunca sabia ano certo ...   \n",
       "2       Eu sorria bastante, mas nunca sabia co certo o...   \n",
       "3       Eu sorria bastante, mas nunca sabia so certo o...   \n",
       "4       Eu sorria bastante, mas nunca sabia Ao certo o...   \n",
       "...                                                   ...   \n",
       "413971  Bem, só Deus criara as luzes verdes, por que n...   \n",
       "413972  Bem, Se Deus criara as luzes verdes, por que n...   \n",
       "413973  Bem, s Deus criara as luzes verdes, por que nã...   \n",
       "413974  Bem, sei Deus criara as luzes verdes, por que ...   \n",
       "413975  Bem, se Deus criara as luzes verdes, por que n...   \n",
       "\n",
       "                                             correct_text  \n",
       "0       Eu sorria bastante, mas nunca sabia ao certo o...  \n",
       "1       Eu sorria bastante, mas nunca sabia ao certo o...  \n",
       "2       Eu sorria bastante, mas nunca sabia ao certo o...  \n",
       "3       Eu sorria bastante, mas nunca sabia ao certo o...  \n",
       "4       Eu sorria bastante, mas nunca sabia ao certo o...  \n",
       "...                                                   ...  \n",
       "413971  Bem, se Deus criara as luzes verdes, por que n...  \n",
       "413972  Bem, se Deus criara as luzes verdes, por que n...  \n",
       "413973  Bem, se Deus criara as luzes verdes, por que n...  \n",
       "413974  Bem, se Deus criara as luzes verdes, por que n...  \n",
       "413975  Bem, se Deus criara as luzes verdes, por que n...  \n",
       "\n",
       "[413976 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated_data.pickle', 'wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
