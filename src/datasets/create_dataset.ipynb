{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "from ebooklib import epub\n",
    "import random, pickle, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brazilian alphabet\n",
    "lower_case = r'abcdefghijklmnopqrstuvwxyzáàâãéêíóôõúç'\n",
    "upper_case = r'ABCDEFGHIJKLMNOPQRSTUVWXYZÁÀÂÃÉÊÍÓÔÕÚÇ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../data/portuguese_sentences.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carolina = load_dataset('carolina-c4ai/corpus-carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carolina_text = carolina['corpus']['text']\n",
    "print(len(carolina_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_books = ['a_guerra_dos_tronos','linha_d_agua','o_alienista', 'ensaio_sobre_a_cegueira', 'sapiens', 'o_guarani', 'colecao_especial_jane_austen', 'o_livro_das_princesas','a_falencia', 'sob_a_redoma', 'os_cem_melhores_contos_brasileiros_do_seculo']\n",
    "#list_books = ['os_tres_mosqueteiros', 'harry_potter_e_a_ordem_da_fenix', 'grande_sertao_veredas', 'a_redoma_de_vidro', 'aristoteles_e_dante_descobrem_os_segredos_do_universo', 'como_evitar_preocupacoes_e_comecar_a_viver']\n",
    "list_books = [book+'.epub' for book in list_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book(book_name):\n",
    "    book = epub.read_epub(f'../../data/epubs/{book_name}')\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    def chapter_to_str(chapter):\n",
    "        soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
    "        text = [para.get_text() for para in soup.find_all('p')]\n",
    "        return ''.join(text)\n",
    "    texts = \"\"\n",
    "    for c in items:\n",
    "        chapter = chapter_to_str(c)\n",
    "        texts += chapter\n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ' '.join([process_book(book) for book in list_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\.|\\?|!|;|\\n'\n",
    "sentences = re.split(regex, raw_text)\n",
    "for carol in carolina_text:\n",
    "    splits = re.split(regex, carol)\n",
    "    sentences.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ s for s in sentences if len(s) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)\n",
    "sentences = sentences[:3_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_sentence(str: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Receives a sentence and returns a list\n",
    "    of all sentences with delimiter upperLower.\n",
    "    Ex.: 'GeorgeMartin' -> ['George', 'Martin']\n",
    "    \"\"\"\n",
    "    split_regex = rf'(?<=[{lower_case}])(?=[{upper_case}])'\n",
    "    return re.split(split_regex, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(str: str) -> str:\n",
    "    str.replace('\\n', ' ')\n",
    "\n",
    "    while str and str[0] in ['.', ',', ':', '!', '?', ';']:\n",
    "        str = str[1:]\n",
    "\n",
    "    # fix whitespaces\n",
    "    while '  ' in str:\n",
    "        str = str.replace('  ', ' ')\n",
    "    if str and str[0] == ' ':\n",
    "        str = str[1:]\n",
    "    if str and str[-1] == ' ':\n",
    "        str = str[:-1]\n",
    "    \n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_concatenate(func, args):\n",
    "    \"\"\"\n",
    "    Receives a function and a list of arguments to the function.\n",
    "    Returns the concatenation of func(args[0])+func(args[1])...\n",
    "    \"\"\"\n",
    "    to_return = []\n",
    "    for obj in args:\n",
    "        to_return.extend(func(obj))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(str: str) -> list[str]:\n",
    "    str = clean_sentence(str)\n",
    "    \n",
    "    # get rid of empty and \n",
    "    # one-letter sentences\n",
    "    if len(str) <= 1:\n",
    "        return []\n",
    "\n",
    "    # base of recursion\n",
    "    splits = chop_sentence(str)\n",
    "\n",
    "    if len(splits) == 1:\n",
    "        return splits\n",
    "    \n",
    "    return apply_and_concatenate(normalize_sentence, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = apply_and_concatenate(normalize_sentence, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=set()\n",
    "print(f'Size with duplicates: {len(sentences)}')\n",
    "for s in sentences:\n",
    "    duplicates.add(s) \n",
    "sentences = list(duplicates)\n",
    "print(f'Size without duplicates: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path, 'w') as file:\n",
    "#     file.write('\\n'.join(sentences))\n",
    "#     file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate annotated data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for generating similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_adjacent_letters_pt = {\n",
    "    'a': ['s', 'z', 'q', 'w', 'á', 'à', 'â', 'ã'],\n",
    "    'b': ['v', 'g', 'n', 'h'],\n",
    "    'c': ['x', 'd', 'v', 'f', 'ç'],\n",
    "    'd': ['s', 'e', 'c', 'x', 'f', 'r'],\n",
    "    'e': ['w', 'r', 'd', 's', 'é', 'ê'],\n",
    "    'f': ['d', 'r', 'g', 'v', 'c', 't'],\n",
    "    'g': ['f', 't', 'h', 'b', 'v', 'r'],\n",
    "    'h': ['g', 't', 'j', 'n', 'b', 'y'],\n",
    "    'i': ['u', 'o', 'k', 'j', 'í'],\n",
    "    'j': ['h', 'y', 'k', 'n', 'm', 'u', 'i'],\n",
    "    'k': ['j', 'i', 'l', 'm', 'o', 'n'],\n",
    "    'l': ['k', 'o', 'p', 'm'],\n",
    "    'm': ['n', 'j', 'k', 'l'],\n",
    "    'n': ['b', 'h', 'j', 'm'],\n",
    "    'o': ['i', 'p', 'l', 'k', 'ó', 'ô', 'õ'],\n",
    "    'p': ['o', 'l', 'ç'],\n",
    "    'q': ['a', 'z', 'u'],\n",
    "    'r': ['e', 't', 'f', 'd', 'r'],\n",
    "    's': ['a', 'w', 'e', 'd', 'x', 'z'],\n",
    "    't': ['r', 'y', 'g', 'f'],\n",
    "    'u': ['y', 'j', 'i', 'h', 'ú'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'a', 's', 'e'],\n",
    "    'x': ['z', 's', 'd', 'c'],\n",
    "    'y': ['t', 'u', 'h', 'g'],\n",
    "    'z': ['x', 's', 'a', 'ç'],\n",
    "    'ç': ['c'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_strings(str, x = None, adjacent_letters = True):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of similar strings,\n",
    "    all in lowercase, according to the following rules:\n",
    "\n",
    "    * if 'x' is None, it will be:\n",
    "      -> 1, if len(str) <= 6\n",
    "      -> 2, if len(str) <= 12\n",
    "      -> 3, if len(str) > 12\n",
    "    * all strings will be common Portuguese cognitive erros or\n",
    "      strings 'x' edits away from str, where an edit is:\n",
    "      -> insert a letter\n",
    "      -> delete a letter\n",
    "      -> replace one letter, and the letter will be any letter in the Portuguese alphabet or \n",
    "         just the adjacent letters in the keyboard if the flag\n",
    "         'adjacent_letters' is set to true.\n",
    "    \"\"\"\n",
    "    str = str.lower()\n",
    "\n",
    "    if x is None:\n",
    "        x = 2\n",
    "        for edits, size in [(1, 6), (2, 12)]:\n",
    "            if len(str) <= size:\n",
    "                x = edits \n",
    "                break\n",
    "\n",
    "    ALPHABET_UPPER = 'ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÃÇÉÊÍÓÔÕÚàáâãçéêíóôõú'\n",
    "    ALPHABET_LOWER = ALPHABET_UPPER.lower()\n",
    "\n",
    "    def concatenate_function(func, n):\n",
    "        if n == 1:\n",
    "            return func\n",
    "        return lambda x: func(concatenate_function(func, n-1)(x))\n",
    "    \n",
    "    def insert(words):\n",
    "        \"\"\"\n",
    "        Receives an iterable of words and returns\n",
    "        a set with all the possible insertions of each word.\n",
    "        \"\"\"\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for pos in range(len(str)+1):\n",
    "                left = str[:pos]\n",
    "                right = str[pos:]\n",
    "                \n",
    "                for char in ALPHABET_LOWER:\n",
    "                    return_words.add(left+char+right)\\\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def delete(words):\n",
    "        return_words = set()\n",
    "        \n",
    "        for str in words:\n",
    "            if len(str) <= 1:\n",
    "                continue \n",
    "            for i in range(len(str)):\n",
    "                left = str[:i]\n",
    "                right = str[i+1:]\n",
    "                return_words.add(left+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def replace(words):\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for ix, char in enumerate(str):\n",
    "                left = str[:ix]\n",
    "                right = str[ix+1:]\n",
    "                for c in ALPHABET_LOWER:\n",
    "                    return_words.add(left+c+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    all_edits = set()\n",
    "\n",
    "    for func in [insert, delete, replace]:\n",
    "        conc_func = concatenate_function(func, x)\n",
    "        all_edits = all_edits | conc_func({str})\n",
    "\n",
    "    for ix,c in enumerate(str):\n",
    "        all_edits = all_edits | {str[:ix]+c.swapcase()+str[ix+1:]}\n",
    "\n",
    "    # common Portuguese errors\n",
    "    # ss and ç\n",
    "    all_edits.add(str.replace('ss', 'ç'))\n",
    "    all_edits.add(str.replace('ç', 'ss'))\n",
    "\n",
    "    # ão and am\n",
    "    all_edits.add(str.replace('ão', 'am'))\n",
    "    all_edits.add(str.replace('am', 'ão'))\n",
    "    \n",
    "    all_edits.discard(str)\n",
    "    \n",
    "    return all_edits\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "\n",
    "# with open(path, 'r') as file:\n",
    "#     for s in file:\n",
    "#         sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/dictionary.csv', 'rb') as file:\n",
    "    loaded_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "freq_dic = {}\n",
    "\n",
    "for word, freq in zip(loaded_df['word'], loaded_df['frequency']):\n",
    "    words.append((-freq, str(word)))\n",
    "    freq_dic[word] = freq\n",
    "\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_mistakes(word, just_similar = False):\n",
    "    if word not in freq_dic:\n",
    "        return []\n",
    "    similar_words = get_similar_strings(word)\n",
    "\n",
    "    if just_similar:\n",
    "        return similar_words\n",
    "\n",
    "    mistakes = []\n",
    "\n",
    "    for similar in similar_words:\n",
    "        if freq_dic.get(similar, 10_000_000) <= (freq_dic[word]/5):\n",
    "            mistakes.append(similar)\n",
    "    mistakes.sort(key=lambda x: freq_dic[x], reverse=True)\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_possible_mistakes('você', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_number(left: int, right: int):\n",
    "    \"\"\"\n",
    "    Requires left <= right.\n",
    "    Returns a random number in the inverval (left, right).\n",
    "    \"\"\"\n",
    "    num = random.random() * (right-left)\n",
    "    num = round(num)\n",
    "    return num+left \n",
    "\n",
    "def draw_random_quantity():\n",
    "    qtd_array = [0,0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,4]\n",
    "    random_index = draw_random_number(0,len(qtd_array)-1)\n",
    "    return qtd_array[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_random_number(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches all lower case words or word with the first upper character and hiphenized words\n",
    "reg = rf'\\b(?:[{upper_case}][{lower_case}]*|[{lower_case}]+(?:-[{lower_case}]+)*|[{lower_case}]*[{upper_case}](?=[{lower_case}]))\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(reg,'Olá guarda-chuva Guarda-chuva paçoca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'wrong_text': [], 'correct_text': []}\n",
    "\n",
    "# aliases\n",
    "wrong_text = df['wrong_text']\n",
    "correct_text = df['correct_text']\n",
    "duplicates = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_data(wrong, right):\n",
    "    if (wrong,right) in duplicates:\n",
    "        return \n",
    "    duplicates.add((wrong,right))\n",
    "    wrong_text.append(wrong)\n",
    "    correct_text.append(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(sentences):    \n",
    "    all_words = list(re.finditer(rf'[{lower_case+upper_case}]+', s))\n",
    "    if len(all_words) > 10:\n",
    "        continue\n",
    "    \n",
    "    # if there are no words, there's nothing to corrupt\n",
    "    if len(all_words) == 0:\n",
    "        continue\n",
    "\n",
    "    # amount of words to mess up in the sentence\n",
    "    to_mess_up = draw_random_number(1,5)\n",
    "\n",
    "    # i have to keep a current for when\n",
    "    # i corrupted a word and am going\n",
    "    # to the next one\n",
    "    curr_s = s\n",
    "\n",
    "    for i in range(to_mess_up):\n",
    "        all_matches = list(re.finditer(rf'[{lower_case+upper_case}]+', curr_s))\n",
    "        \n",
    "        if not all_matches:\n",
    "            break\n",
    "\n",
    "        # get random word to corrupt  \n",
    "        random_ix = draw_random_number(0,len(all_matches)-1)\n",
    "        match=all_matches[random_ix]\n",
    "        \n",
    "        # get all of its mistakes\n",
    "        mistakes = get_possible_mistakes(match.group())\n",
    "        if not mistakes:\n",
    "            # nothing to see here\n",
    "            continue \n",
    "\n",
    "        # 15 most frequent\n",
    "        mistakes = mistakes[:15]\n",
    "        random.shuffle(mistakes)\n",
    "        mistake = mistakes[0]\n",
    "\n",
    "        # get word boundaries in the\n",
    "        # sentence\n",
    "        beg = match.start()\n",
    "        en = match.end()\n",
    "\n",
    "        curr_s = curr_s[:beg]+mistake+curr_s[en:]\n",
    "    add_to_data(curr_s, s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,c in zip(wrong_text[:5], correct_text[:5]):\n",
    "    print(w)\n",
    "    print(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['wrong_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.push_to_hub(\"carolmou/dataset-1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
