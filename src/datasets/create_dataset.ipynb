{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "from ebooklib import epub\n",
    "import random, pickle, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../data/portuguese_sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brazilian alphabet\n",
    "lower_case = r'abcdefghijklmnopqrstuvwxyzáàâãéêíóôõúç'\n",
    "upper_case = r'ABCDEFGHIJKLMNOPQRSTUVWXYZÁÀÂÃÉÊÍÓÔÕÚÇ'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset corpus-carolina (/home/carolmou/.cache/huggingface/datasets/carolina-c4ai___corpus-carolina/carolina/1.2.0/60fe73ac1719891e34135322031692bf177e9323e830d620cf3304f535ee2693)\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.70it/s]\n"
     ]
    }
   ],
   "source": [
    "carolina = load_dataset('carolina-c4ai/corpus-carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2107045\n"
     ]
    }
   ],
   "source": [
    "carolina_text = carolina['corpus']['text']\n",
    "print(len(carolina_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_books = ['a_guerra_dos_tronos','linha_d_agua','o_alienista', 'ensaio_sobre_a_cegueira', 'sapiens', 'o_guarani', 'colecao_especial_jane_austen', 'o_livro_das_princesas','a_falencia', 'sob_a_redoma', 'os_cem_melhores_contos_brasileiros_do_seculo']\n",
    "#list_books = ['os_tres_mosqueteiros', 'harry_potter_e_a_ordem_da_fenix', 'grande_sertao_veredas', 'a_redoma_de_vidro', 'aristoteles_e_dante_descobrem_os_segredos_do_universo', 'como_evitar_preocupacoes_e_comecar_a_viver']\n",
    "list_books = [book+'.epub' for book in list_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book(book_name):\n",
    "    book = epub.read_epub(f'../../data/epubs/{book_name}')\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    def chapter_to_str(chapter):\n",
    "        soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
    "        text = [para.get_text() for para in soup.find_all('p')]\n",
    "        return ''.join(text)\n",
    "    texts = \"\"\n",
    "    for c in items:\n",
    "        chapter = chapter_to_str(c)\n",
    "        texts += chapter\n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolmou/linguas-indigenas/.venv/lib/python3.10/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "raw_text = ' '.join([process_book(book) for book in list_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\.|\\?|!|;|\\n'\n",
    "sentences = re.split(regex, raw_text)\n",
    "for carol in carolina_text:\n",
    "    splits = re.split(regex, carol)\n",
    "    sentences.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ s for s in sentences if len(s) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)\n",
    "sentences = sentences[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_sentence(str: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Receives a sentence and returns a list\n",
    "    of all sentences with delimiter upperLower.\n",
    "    Ex.: 'GeorgeMartin' -> ['George', 'Martin']\n",
    "    \"\"\"\n",
    "    split_regex = rf'(?<=[{lower_case}])(?=[{upper_case}])'\n",
    "    return re.split(split_regex, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(str: str) -> str:\n",
    "    str.replace('\\n', ' ')\n",
    "\n",
    "    while str and str[0] in ['.', ',', ':', '!', '?', ';']:\n",
    "        str = str[1:]\n",
    "\n",
    "    # fix whitespaces\n",
    "    while '  ' in str:\n",
    "        str = str.replace('  ', ' ')\n",
    "    if str and str[0] == ' ':\n",
    "        str = str[1:]\n",
    "    if str and str[-1] == ' ':\n",
    "        str = str[:-1]\n",
    "    \n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_concatenate(func, args):\n",
    "    \"\"\"\n",
    "    Receives a function and a list of arguments to the function.\n",
    "    Returns the concatenation of func(args[0])+func(args[1])...\n",
    "    \"\"\"\n",
    "    to_return = []\n",
    "    for obj in args:\n",
    "        to_return.extend(func(obj))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(str: str) -> list[str]:\n",
    "    str = clean_sentence(str)\n",
    "    \n",
    "    # get rid of empty and \n",
    "    # one-letter sentences\n",
    "    if len(str) <= 1:\n",
    "        return []\n",
    "\n",
    "    # base of recursion\n",
    "    splits = chop_sentence(str)\n",
    "\n",
    "    if len(splits) == 1:\n",
    "        return splits\n",
    "    \n",
    "    return apply_and_concatenate(normalize_sentence, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = apply_and_concatenate(normalize_sentence, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size with duplicates: 1015455\n",
      "Size without duplicates: 778237\n"
     ]
    }
   ],
   "source": [
    "duplicates=set()\n",
    "print(f'Size with duplicates: {len(sentences)}')\n",
    "for s in sentences:\n",
    "    duplicates.add(s) \n",
    "sentences = list(duplicates)\n",
    "print(f'Size without duplicates: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path, 'w') as file:\n",
    "#     file.write('\\n'.join(sentences))\n",
    "#     file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate annotated data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for generating similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_adjacent_letters_pt = {\n",
    "    'a': ['s', 'z', 'q', 'w', 'á', 'à', 'â', 'ã'],\n",
    "    'b': ['v', 'g', 'n', 'h'],\n",
    "    'c': ['x', 'd', 'v', 'f', 'ç'],\n",
    "    'd': ['s', 'e', 'c', 'x', 'f', 'r'],\n",
    "    'e': ['w', 'r', 'd', 's', 'é', 'ê'],\n",
    "    'f': ['d', 'r', 'g', 'v', 'c', 't'],\n",
    "    'g': ['f', 't', 'h', 'b', 'v', 'r'],\n",
    "    'h': ['g', 't', 'j', 'n', 'b', 'y'],\n",
    "    'i': ['u', 'o', 'k', 'j', 'í'],\n",
    "    'j': ['h', 'y', 'k', 'n', 'm', 'u', 'i'],\n",
    "    'k': ['j', 'i', 'l', 'm', 'o', 'n'],\n",
    "    'l': ['k', 'o', 'p', 'm'],\n",
    "    'm': ['n', 'j', 'k', 'l'],\n",
    "    'n': ['b', 'h', 'j', 'm'],\n",
    "    'o': ['i', 'p', 'l', 'k', 'ó', 'ô', 'õ'],\n",
    "    'p': ['o', 'l', 'ç'],\n",
    "    'q': ['a', 'z', 'u'],\n",
    "    'r': ['e', 't', 'f', 'd', 'r'],\n",
    "    's': ['a', 'w', 'e', 'd', 'x', 'z'],\n",
    "    't': ['r', 'y', 'g', 'f'],\n",
    "    'u': ['y', 'j', 'i', 'h', 'ú'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'a', 's', 'e'],\n",
    "    'x': ['z', 's', 'd', 'c'],\n",
    "    'y': ['t', 'u', 'h', 'g'],\n",
    "    'z': ['x', 's', 'a', 'ç'],\n",
    "    'ç': ['c'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_strings(str, x = None, adjacent_letters = True):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns a list of similar strings,\n",
    "    all in lowercase, according to the following rules:\n",
    "\n",
    "    * if 'x' is None, it will be:\n",
    "      -> 1, if len(str) <= 6\n",
    "      -> 2, if len(str) <= 12\n",
    "      -> 3, if len(str) > 12\n",
    "    * all strings will be common Portuguese cognitive erros or\n",
    "      strings 'x' edits away from str, where an edit is:\n",
    "      -> insert a letter\n",
    "      -> delete a letter\n",
    "      -> replace one letter, and the letter will be any letter in the Portuguese alphabet or \n",
    "         just the adjacent letters in the keyboard if the flag\n",
    "         'adjacent_letters' is set to true.\n",
    "    \"\"\"\n",
    "    str = str.lower()\n",
    "\n",
    "    if x is None:\n",
    "        x = 2\n",
    "        for edits, size in [(1, 6), (2, 12)]:\n",
    "            if len(str) <= size:\n",
    "                x = edits \n",
    "                break\n",
    "\n",
    "    ALPHABET_UPPER = 'ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÃÇÉÊÍÓÔÕÚàáâãçéêíóôõú'\n",
    "    ALPHABET_LOWER = ALPHABET_UPPER.lower()\n",
    "\n",
    "    def concatenate_function(func, n):\n",
    "        if n == 1:\n",
    "            return func\n",
    "        return lambda x: func(concatenate_function(func, n-1)(x))\n",
    "    \n",
    "    def insert(words):\n",
    "        \"\"\"\n",
    "        Receives an iterable of words and returns\n",
    "        a set with all the possible insertions of each word.\n",
    "        \"\"\"\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for pos in range(len(str)+1):\n",
    "                left = str[:pos]\n",
    "                right = str[pos:]\n",
    "                \n",
    "                for char in ALPHABET_LOWER:\n",
    "                    return_words.add(left+char+right)\\\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def delete(words):\n",
    "        return_words = set()\n",
    "        \n",
    "        for str in words:\n",
    "            if len(str) <= 1:\n",
    "                continue \n",
    "            for i in range(len(str)):\n",
    "                left = str[:i]\n",
    "                right = str[i+1:]\n",
    "                return_words.add(left+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    def replace(words):\n",
    "        return_words = set()\n",
    "\n",
    "        for str in words:\n",
    "            for ix, char in enumerate(str):\n",
    "                left = str[:ix]\n",
    "                right = str[ix+1:]\n",
    "                for c in ALPHABET_LOWER:\n",
    "                    return_words.add(left+c+right)\n",
    "        \n",
    "        return return_words\n",
    "    \n",
    "    all_edits = set()\n",
    "\n",
    "    for func in [insert, delete, replace]:\n",
    "        conc_func = concatenate_function(func, x)\n",
    "        all_edits = all_edits | conc_func({str})\n",
    "\n",
    "    for ix,c in enumerate(str):\n",
    "        all_edits = all_edits | {str[:ix]+c.swapcase()+str[ix+1:]}\n",
    "\n",
    "    # common Portuguese errors\n",
    "    # ss and ç\n",
    "    all_edits.add(str.replace('ss', 'ç'))\n",
    "    all_edits.add(str.replace('ç', 'ss'))\n",
    "\n",
    "    # ão and am\n",
    "    all_edits.add(str.replace('ão', 'am'))\n",
    "    all_edits.add(str.replace('am', 'ão'))\n",
    "    \n",
    "    all_edits.discard(str)\n",
    "    \n",
    "    return all_edits\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "\n",
    "# with open(path, 'r') as file:\n",
    "#     for s in file:\n",
    "#         sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/dictionary.csv', 'rb') as file:\n",
    "    loaded_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ficha</td>\n",
       "      <td>6422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>George</td>\n",
       "      <td>52633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>460650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>os</td>\n",
       "      <td>4143352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>direitos</td>\n",
       "      <td>118656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059672</th>\n",
       "      <td>3059672</td>\n",
       "      <td>Frattaminore</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059673</th>\n",
       "      <td>3059673</td>\n",
       "      <td>Cellole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059674</th>\n",
       "      <td>3059674</td>\n",
       "      <td>Casavatore</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059675</th>\n",
       "      <td>3059675</td>\n",
       "      <td>sivitit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059676</th>\n",
       "      <td>3059676</td>\n",
       "      <td>Esterino</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3059677 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0          word  frequency\n",
       "0                 0         Ficha       6422\n",
       "1                 1        George      52633\n",
       "2                 2             R     460650\n",
       "3                 3            os    4143352\n",
       "4                 4      direitos     118656\n",
       "...             ...           ...        ...\n",
       "3059672     3059672  Frattaminore          5\n",
       "3059673     3059673       Cellole          1\n",
       "3059674     3059674    Casavatore          2\n",
       "3059675     3059675       sivitit          1\n",
       "3059676     3059676      Esterino          1\n",
       "\n",
       "[3059677 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "freq_dic = {}\n",
    "\n",
    "for word, freq in zip(loaded_df['word'], loaded_df['frequency']):\n",
    "    words.append((-freq, str(word)))\n",
    "    freq_dic[word] = freq\n",
    "\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_mistakes(word, just_similar = False):\n",
    "    if word not in freq_dic:\n",
    "        return []\n",
    "    similar_words = get_similar_strings(word)\n",
    "\n",
    "    if just_similar:\n",
    "        return similar_words\n",
    "\n",
    "    mistakes = []\n",
    "\n",
    "    for similar in similar_words:\n",
    "        if freq_dic.get(similar, 10_000_000) <= (freq_dic[word]/5):\n",
    "            mistakes.append(similar)\n",
    "    mistakes.sort(key=lambda x: freq_dic[x], reverse=True)\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vopcê', 'bocê', 'voãcê', 'voãê', 'vúocê', 'vocc', 'vocêe', 'vâcê', 'vmocê', 'voàcê', 'hocê', 'ôocê', 'vãcê', 'vcocê', 'vocy', 'ávocê', 'vocêã', 'vocé', 'vqcê', 'vkcê', 'vêcê', 'voc', 'vdocê', 'ovocê', 'voecê', 'vsocê', 'vçocê', 'çocê', 'focê', 'úocê', 'vvocê', 'õocê', 'vàocê', 'vocjê', 'kvocê', 'vscê', 'vocxê', 'qvocê', 'vocz', 'vxocê', 'nvocê', 'vcê', 'rocê', 'cvocê', 'vmcê', 'wocê', 'volê', 'vocwê', 'vocw', 'votê', 'vocrê', 'váocê', 'vowê', 'vjocê', 'vocêé', 'tocê', 'voôê', 'vocêd', 'évocê', 'vocf', 'voeê', 'voácê', 'bvocê', 'voóê', 'vocp', 'vocêq', 'vobê', 'vncê', 'vrocê', 'vocú', 'íocê', 'voõcê', 'voaê', 'vocêc', 'vokcê', 'vocêa', 'vociê', 'vrcê', 'voucê', 'vocêâ', 'vpcê', 'vocêx', 'voócê', 'voxê', 'voicê', 'vocêí', 'ãocê', 'vocên', 'vocãê', 'voáê', 'vogê', 'xocê', 'óvocê', 'voxcê', 'vocpê', 'vocêt', 'vocêç', 'uvocê', 'vócê', 'vockê', 'voiê', 'vouê', 'vocóê', 'vocêb', 'vocaê', 'vocêg', 'vâocê', 'vOcê', 'vúcê', 'vecê', 'vtocê', 'svocê', 'âvocê', 'vocêr', 'vofcê', 'vocêm', 'ivocê', 'voCê', 'vocÊ', 'vwcê', 'voyê', 'voàê', 'vocb', 'voqê', 'voéê', 'aocê', 'vocíê', 'avocê', 'vjcê', 'vocêô', 'voczê', 'vocdê', 'voêê', 'vock', 'tvocê', 'vgocê', 'vêocê', 'êvocê', 'veocê', 'vpocê', 'vooê', 'vochê', 'wvocê', 'vécê', 'vóocê', 'vqocê', 'vocêu', 'vosê', 'vacê', 'voâcê', 'vaocê', 'vocçê', 'vorcê', 'nocê', 'vocáê', 'vocêp', 'voccê', 'voci', 'vocyê', 'viocê', 'iocê', 'vomê', 'voct', 'âocê', 'vocêá', 'zocê', 'vlcê', 'vocêê', 'vbcê', 'voícê', 'voctê', 'docê', 'vocô', 'vgcê', 'vicê', 'voca', 'vocã', 'vodê', 'vxcê', 'vofê', 'vocó', 'vowcê', 'vwocê', 'vocuê', 'vyocê', 'voycê', 'àocê', 'éocê', 'voce', 'vícê', 'votcê', 'lvocê', 'voceê', 'dvocê', 'víocê', 'võcê', 'vocbê', 'vopê', 'vocg', 'vzcê', 'êocê', 'vokê', 'vácê', 'vdcê', 'hvocê', 'voacê', 'voê', 'gocê', 'voécê', 'Você', 'voíê', 'vkocê', 'vodcê', 'vonê', 'voúcê', 'socê', 'vfcê', 'vocfê', 'mocê', 'ãvocê', 'vocúê', 'yocê', 'ôvocê', 'vnocê', 'vocnê', 'vojê', 'úvocê', 'vocêl', 'vocêk', 'vycê', 'vàcê', 'gvocê', 'voocê', 'vovcê', 'voscê', 'óocê', 'vocvê', 'jocê', 'vhocê', 'cocê', 'vocx', 'vocâ', 'vocêà', 'vocôê', 'vocqê', 'voçê', 'vôocê', 'vovê', 'vocm', 'vôcê', 'vfocê', 'pvocê', 'àvocê', 'vccê', 'yvocê', 'eocê', 'vocsê', 'uocê', 'vzocê', 'rvocê', 'vãocê', 'vocêh', 'xvocê', 'vocêú', 'vocêv', 'vlocê', 'vohê', 'vbocê', 'voúê', 'vocêo', 'õvocê', 'voçcê', 'vocéê', 'zvocê', 'evocê', 'kocê', 'vocêj', 'oocê', 'vuocê', 'vocêf', 'vogcê', 'vobcê', 'áocê', 'vocêz', 'vtcê', 'võocê', 'pocê', 'voõê', 'locê', 'qocê', 'véocê', 'vocàê', 'vocêõ', 'vocà', 'vozê', 'vocr', 'vocõê', 'vocl', 'vomcê', 'voncê', 'vocâê', 'volcê', 'vocêw', 'voco', 'vocõ', 'vozcê', 'vvcê', 'vocu', 'vorê', 'vucê', 'voqcê', 'vohcê', 'vocd', 'fvocê', 'vocêy', 'vocq', 'vocá', 'vhcê', 'vocj', 'jvocê', 'vocgê', 'voêcê', 'voch', 'vocs', 'vocêó', 'vocoê', 'vocí', 'vojcê', 'vocç', 'vocn', 'ocê', 'ívocê', 'voôcê', 'çvocê', 'voâê', 'vocêi', 'mvocê', 'vçcê', 'vocês', 'vocv', 'vocmê', 'voclê'}\n"
     ]
    }
   ],
   "source": [
    "print(get_possible_mistakes('você', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_number(left: int, right: int):\n",
    "    \"\"\"\n",
    "    Requires left <= right.\n",
    "    Returns a random number in the inverval (left, right).\n",
    "    \"\"\"\n",
    "    num = random.random() * (right-left)\n",
    "    num = round(num)\n",
    "    return num+left \n",
    "\n",
    "def draw_random_quantity():\n",
    "    qtd_array = [0,0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,4]\n",
    "    random_index = draw_random_number(0,len(qtd_array)-1)\n",
    "    return qtd_array[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_random_number(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches all lower case words or word with the first upper character and hiphenized words\n",
    "reg = rf'\\b(?:[{upper_case}][{lower_case}]*|[{lower_case}]+(?:-[{lower_case}]+)*|[{lower_case}]*[{upper_case}](?=[{lower_case}]))\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Olá', 'guarda-chuva', 'Guarda', 'chuva', 'paçoca']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(reg,'Olá guarda-chuva Guarda-chuva paçoca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'wrong_text': [], 'correct_text': []}\n",
    "\n",
    "# aliases\n",
    "wrong_text = df['wrong_text']\n",
    "correct_text = df['correct_text']\n",
    "duplicates = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_data(wrong, right):\n",
    "    if (wrong,right) in duplicates:\n",
    "        return \n",
    "    duplicates.add((wrong,right))\n",
    "    wrong_text.append(wrong)\n",
    "    correct_text.append(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/778237 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 429035/778237 [11:52:05<9:39:34, 10.04it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m match\u001b[39m=\u001b[39mall_matches[random_ix]\n\u001b[1;32m     28\u001b[0m \u001b[39m# get all of its mistakes\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m mistakes \u001b[39m=\u001b[39m get_possible_mistakes(match\u001b[39m.\u001b[39;49mgroup())\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mistakes:\n\u001b[1;32m     31\u001b[0m     \u001b[39m# nothing to see here\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39mcontinue\u001b[39;00m \n",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m, in \u001b[0;36mget_possible_mistakes\u001b[0;34m(word, just_similar)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m freq_dic:\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m----> 4\u001b[0m similar_words \u001b[39m=\u001b[39m get_similar_strings(word)\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m just_similar:\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m similar_words\n",
      "Cell \u001b[0;32mIn[26], line 81\u001b[0m, in \u001b[0;36mget_similar_strings\u001b[0;34m(str, x, adjacent_letters)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m [insert, delete, replace]:\n\u001b[1;32m     80\u001b[0m     conc_func \u001b[39m=\u001b[39m concatenate_function(func, x)\n\u001b[0;32m---> 81\u001b[0m     all_edits \u001b[39m=\u001b[39m all_edits \u001b[39m|\u001b[39m conc_func({\u001b[39mstr\u001b[39;49m})\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m ix,c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mstr\u001b[39m):\n\u001b[1;32m     84\u001b[0m     all_edits \u001b[39m=\u001b[39m all_edits \u001b[39m|\u001b[39m {\u001b[39mstr\u001b[39m[:ix]\u001b[39m+\u001b[39mc\u001b[39m.\u001b[39mswapcase()\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m[ix\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]}\n",
      "Cell \u001b[0;32mIn[26], line 33\u001b[0m, in \u001b[0;36mget_similar_strings.<locals>.concatenate_function.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[0;32m---> 33\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m x: func(concatenate_function(func, n\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)(x))\n",
      "Cell \u001b[0;32mIn[26], line 48\u001b[0m, in \u001b[0;36mget_similar_strings.<locals>.insert\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     45\u001b[0m         right \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m[pos:]\n\u001b[1;32m     47\u001b[0m         \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m ALPHABET_LOWER:\n\u001b[0;32m---> 48\u001b[0m             return_words\u001b[39m.\u001b[39;49madd(left\u001b[39m+\u001b[39;49mchar\u001b[39m+\u001b[39;49mright)\\\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m return_words\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for s in tqdm(sentences):    \n",
    "    all_words = list(re.finditer(rf'[{lower_case+upper_case}]+', s))\n",
    "    if len(all_words) > 10:\n",
    "        continue\n",
    "    \n",
    "    # if there are no words, there's nothing to corrupt\n",
    "    if len(all_words) == 0:\n",
    "        continue\n",
    "\n",
    "    # amount of words to mess up in the sentence\n",
    "    to_mess_up = draw_random_number(1,5)\n",
    "\n",
    "    # i have to keep a current for when\n",
    "    # i corrupted a word and am going\n",
    "    # to the next one\n",
    "    curr_s = s\n",
    "\n",
    "    for i in range(to_mess_up):\n",
    "        all_matches = list(re.finditer(rf'[{lower_case+upper_case}]+', curr_s))\n",
    "        \n",
    "        if not all_matches:\n",
    "            break\n",
    "\n",
    "        # get random word to corrupt  \n",
    "        random_ix = draw_random_number(0,len(all_matches)-1)\n",
    "        match=all_matches[random_ix]\n",
    "        \n",
    "        # get all of its mistakes\n",
    "        mistakes = get_possible_mistakes(match.group())\n",
    "        if not mistakes:\n",
    "            # nothing to see here\n",
    "            continue \n",
    "\n",
    "        # 15 most frequent\n",
    "        mistakes = mistakes[:15]\n",
    "        random.shuffle(mistakes)\n",
    "        mistake = mistakes[0]\n",
    "\n",
    "        # get word boundaries in the\n",
    "        # sentence\n",
    "        beg = match.start()\n",
    "        en = match.end()\n",
    "\n",
    "        curr_s = curr_s[:beg]+mistake+curr_s[en:]\n",
    "    add_to_data(curr_s, s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sepultura de pichael Pupin no ceminário de Woodlawn\n",
      "Sepultura de Michael Pupin no Cemitério de Woodlawn\n",
      "\n",
      "outra, quse cancelos\n",
      "outra, quase cancelei\n",
      "\n",
      ") anos, p contar d recedimento definitivo\n",
      ") anos, a contar do recebimento definitivo\n",
      "\n",
      "2/3 des inibiam – grifos no original)\n",
      "2/3 da inicial – grifos no original)\n",
      "\n",
      "(A/d) : cz R l A BADV\n",
      "(A/S) : G R D A BADV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w,c in zip(wrong_text[:5], correct_text[:5]):\n",
    "    print(w)\n",
    "    print(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231224"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['wrong_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wrong_text', 'correct_text'],\n",
       "    num_rows: 231224\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 232/232 [00:00<00:00, 1720.33ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "Downloading metadata: 100%|██████████| 507/507 [00:00<00:00, 3.14MB/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "df.push_to_hub(\"carolmou/dataset-1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
